{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffedf480",
   "metadata": {},
   "source": [
    "# ü§ñ Course Notes Chatbot Demonstration\n",
    "\n",
    "## Assignment 1: Build a Chatbot that Answers from Your Course Notes\n",
    "\n",
    "This notebook demonstrates how to build a complete RAG (Retrieval-Augmented Generation) chatbot using:\n",
    "- **LlamaIndex** for document processing and indexing\n",
    "- **Faiss** for fast vector similarity search  \n",
    "- **Hugging Face Transformers** for embeddings and language models\n",
    "\n",
    "The chatbot will be able to answer questions based on your course notes by finding relevant content and generating contextual responses.\n",
    "\n",
    "### üìã Assignment Objectives\n",
    "‚úÖ Understand chatbot concepts and functionality  \n",
    "‚úÖ Learn to use LlamaIndex, Faiss, and language models  \n",
    "‚úÖ Build a working chatbot that answers from course notes  \n",
    "‚úÖ Test and evaluate the chatbot performance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4ec6a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51c44afd",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Library Installation\n",
    "\n",
    "First, let's install all the required libraries for our chatbot. This includes LlamaIndex for document processing, Faiss for vector search, and Hugging Face transformers for embeddings and language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a759fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Run this cell if you haven't installed the requirements yet\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Install required packages for the chatbot\"\"\"\n",
    "    packages = [\n",
    "        \"llama-index==0.10.62\",\n",
    "        \"llama-index-llms-huggingface==0.2.4\", \n",
    "        \"llama-index-embeddings-huggingface==0.2.2\",\n",
    "        \"llama-index-vector-stores-faiss==0.1.2\",\n",
    "        \"faiss-cpu==1.7.4\",\n",
    "        \"transformers==4.35.2\",\n",
    "        \"torch\",\n",
    "        \"sentence-transformers==2.2.2\",\n",
    "        \"pypdf==3.17.0\",\n",
    "        \"python-docx==1.1.0\",\n",
    "        \"numpy\",\n",
    "        \"pandas\",\n",
    "        \"tqdm\"\n",
    "    ]\n",
    "    \n",
    "    for package in packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            print(f\"‚úÖ Installed {package}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå Failed to install {package}: {e}\")\n",
    "\n",
    "# Uncomment the line below to install packages\n",
    "# install_packages()\n",
    "\n",
    "print(\"üì¶ If you see import errors in the next cells, uncomment the line above and run this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4decbf",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries\n",
    "\n",
    "Now let's import all the necessary libraries for building our chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a824a865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Python libraries\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "import warnings\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# LlamaIndex core components\n",
    "from llama_index.core import VectorStoreIndex, Document, ServiceContext, StorageContext\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.readers.file import PDFReader, DocxReader\n",
    "\n",
    "# Hugging Face components\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "# Vector store\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "import faiss\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"üíª CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"üß† Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e62b3a",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Course Notes\n",
    "\n",
    "Let's create a function to load course notes from various file formats. For this demo, we'll use the sample notes provided, but you can replace these with your actual course materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d7ead7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "course_notes_dir = Path(\"../course_notes\")\n",
    "sample_notes_path = course_notes_dir / \"sample_ml_notes.txt\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "course_notes_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create sample course notes if they don't exist\n",
    "if not sample_notes_path.exists():\n",
    "    sample_content = \"\"\"\n",
    "# Machine Learning Fundamentals\n",
    "\n",
    "## Introduction to Machine Learning\n",
    "Machine Learning is a subset of artificial intelligence that focuses on algorithms and statistical models that enable computers to improve their performance on a task through experience.\n",
    "\n",
    "### Types of Machine Learning\n",
    "1. **Supervised Learning**: Learning with labeled data\n",
    "   - Classification: Predicting categories\n",
    "   - Regression: Predicting continuous values\n",
    "\n",
    "2. **Unsupervised Learning**: Learning without labeled data\n",
    "   - Clustering: Grouping similar data points\n",
    "   - Dimensionality Reduction: Reducing feature space\n",
    "\n",
    "3. **Reinforcement Learning**: Learning through interaction with environment\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Data Preprocessing\n",
    "- **Data Cleaning**: Removing or correcting corrupted data\n",
    "- **Feature Scaling**: Normalizing data ranges\n",
    "- **Feature Selection**: Choosing relevant features\n",
    "\n",
    "### Model Evaluation\n",
    "- **Accuracy**: Percentage of correct predictions\n",
    "- **Precision**: True positives / (True positives + False positives)\n",
    "- **Recall**: True positives / (True positives + False negatives)\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "\n",
    "### Popular Algorithms\n",
    "1. **Linear Regression**: Simple predictive modeling\n",
    "2. **Logistic Regression**: Classification algorithm\n",
    "3. **Decision Trees**: Tree-like model for decisions\n",
    "4. **Random Forest**: Ensemble of decision trees\n",
    "5. **Support Vector Machines**: Finding optimal decision boundaries\n",
    "6. **Neural Networks**: Inspired by biological neural networks\n",
    "\n",
    "## Important Definitions\n",
    "\n",
    "**Overfitting**: When a model performs well on training data but poorly on new data.\n",
    "\n",
    "**Underfitting**: When a model is too simple to capture the underlying pattern.\n",
    "\n",
    "**Bias-Variance Tradeoff**: Balance between model's ability to minimize bias and variance.\n",
    "\n",
    "**Gradient Descent**: Optimization algorithm used to minimize loss functions.\n",
    "\n",
    "## Practical Applications\n",
    "- Image Recognition\n",
    "- Natural Language Processing  \n",
    "- Recommendation Systems\n",
    "- Fraud Detection\n",
    "- Medical Diagnosis\n",
    "\"\"\"\n",
    "    \n",
    "    with open(sample_notes_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(sample_content)\n",
    "    print(f\"‚úÖ Created sample course notes at {sample_notes_path}\")\n",
    "\n",
    "# Load documents function\n",
    "def load_documents_from_directory(directory: Path) -> List[Document]:\n",
    "    \"\"\"Load all supported documents from a directory\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    # Supported file types\n",
    "    loaders = {\n",
    "        '.txt': lambda p: [Document(text=p.read_text(encoding='utf-8'), \n",
    "                                   metadata={\"filename\": p.name, \"file_type\": \"text\"})],\n",
    "        '.pdf': lambda p: PDFReader().load_data(str(p)),\n",
    "        '.docx': lambda p: DocxReader().load_data(str(p))\n",
    "    }\n",
    "    \n",
    "    for file_path in directory.iterdir():\n",
    "        if file_path.is_file() and file_path.suffix.lower() in loaders:\n",
    "            try:\n",
    "                print(f\"üìÑ Loading {file_path.name}...\")\n",
    "                docs = loaders[file_path.suffix.lower()](file_path)\n",
    "                for doc in docs:\n",
    "                    if not hasattr(doc, 'metadata'):\n",
    "                        doc.metadata = {}\n",
    "                    doc.metadata.update({\n",
    "                        \"filename\": file_path.name,\n",
    "                        \"file_path\": str(file_path)\n",
    "                    })\n",
    "                documents.extend(docs)\n",
    "                print(f\"‚úÖ Loaded {len(docs)} chunks from {file_path.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error loading {file_path.name}: {e}\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Load all course notes\n",
    "print(\"üìö Loading course notes...\")\n",
    "documents = load_documents_from_directory(course_notes_dir)\n",
    "print(f\"‚úÖ Loaded {len(documents)} document chunks total\")\n",
    "\n",
    "# Display first document preview\n",
    "if documents:\n",
    "    print(f\"\\nüìñ Preview of first document:\")\n",
    "    print(f\"Filename: {documents[0].metadata.get('filename', 'Unknown')}\")\n",
    "    print(f\"Content preview: {documents[0].text[:300]}...\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No documents loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875b79d8",
   "metadata": {},
   "source": [
    "## 4. Initialize LlamaIndex with Course Documents\n",
    "\n",
    "Now we'll process our documents using LlamaIndex's text splitter to create optimal chunks for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeed58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure document chunking\n",
    "CHUNK_SIZE = 512\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "# Create text splitter\n",
    "text_splitter = SentenceSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP\n",
    ")\n",
    "\n",
    "print(f\"üîß Configured text splitter:\")\n",
    "print(f\"   - Chunk size: {CHUNK_SIZE} tokens\")\n",
    "print(f\"   - Chunk overlap: {CHUNK_OVERLAP} tokens\")\n",
    "\n",
    "# Split documents into chunks\n",
    "print(f\"\\nüìù Processing {len(documents)} documents into chunks...\")\n",
    "nodes = text_splitter.get_nodes_from_documents(documents, show_progress=True)\n",
    "\n",
    "print(f\"‚úÖ Created {len(nodes)} text chunks\")\n",
    "\n",
    "# Display chunk information\n",
    "if nodes:\n",
    "    print(f\"\\nüìä Chunk Statistics:\")\n",
    "    chunk_lengths = [len(node.text) for node in nodes]\n",
    "    print(f\"   - Average chunk length: {np.mean(chunk_lengths):.0f} characters\")\n",
    "    print(f\"   - Min chunk length: {min(chunk_lengths)} characters\")\n",
    "    print(f\"   - Max chunk length: {max(chunk_lengths)} characters\")\n",
    "    \n",
    "    print(f\"\\nüìñ Sample chunk:\")\n",
    "    print(f\"Text: {nodes[0].text[:200]}...\")\n",
    "    print(f\"Metadata: {nodes[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e7543a",
   "metadata": {},
   "source": [
    "## 5. Set Up Faiss Vector Store\n",
    "\n",
    "We'll create a Faiss vector database to enable fast similarity search over our document embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0509c595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up embedding model\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "EMBEDDING_DIM = 384  # Dimension for all-MiniLM-L6-v2\n",
    "\n",
    "print(f\"üß† Setting up embedding model: {EMBEDDING_MODEL}\")\n",
    "\n",
    "# Initialize embedding model\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=EMBEDDING_MODEL,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Embedding model loaded successfully!\")\n",
    "\n",
    "# Create Faiss index\n",
    "print(f\"\\nüóÑÔ∏è Creating Faiss vector store with dimension {EMBEDDING_DIM}\")\n",
    "\n",
    "# Create Faiss index (L2 distance)\n",
    "faiss_index = faiss.IndexFlatL2(EMBEDDING_DIM)\n",
    "\n",
    "print(f\"‚úÖ Faiss index created:\")\n",
    "print(f\"   - Index type: L2 (Euclidean distance)\")\n",
    "print(f\"   - Dimension: {EMBEDDING_DIM}\")\n",
    "print(f\"   - Current size: {faiss_index.ntotal} vectors\")\n",
    "\n",
    "# Create LlamaIndex Faiss vector store\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "print(\"‚úÖ Vector store and storage context ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a20748",
   "metadata": {},
   "source": [
    "## 6. Configure Language Model\n",
    "\n",
    "Now we'll set up a language model for generating responses. We'll use a Hugging Face model that works well for question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c624c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure language model\n",
    "LLM_MODEL = \"microsoft/DialoGPT-medium\"\n",
    "\n",
    "print(f\"ü§ñ Setting up language model: {LLM_MODEL}\")\n",
    "\n",
    "try:\n",
    "    # Initialize language model\n",
    "    llm = HuggingFaceLLM(\n",
    "        model_name=LLM_MODEL,\n",
    "        tokenizer_name=LLM_MODEL,\n",
    "        context_window=1024,\n",
    "        max_new_tokens=256,\n",
    "        model_kwargs={\"torch_dtype\": torch.float16 if torch.cuda.is_available() else torch.float32},\n",
    "        tokenizer_kwargs={},\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "    print(\"‚úÖ Language model loaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error loading language model: {e}\")\n",
    "    print(\"üí° Using default LLM (this may affect response quality)\")\n",
    "    llm = None\n",
    "\n",
    "# Create service context\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    "    node_parser=text_splitter\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Service context created with:\")\n",
    "print(f\"   - LLM: {LLM_MODEL if llm else 'Default'}\")\n",
    "print(f\"   - Embedding: {EMBEDDING_MODEL}\")\n",
    "print(f\"   - Chunk size: {CHUNK_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cde9800",
   "metadata": {},
   "source": [
    "## 7. Build the Query Engine\n",
    "\n",
    "This is where the magic happens! We'll create a vector index from our documents and set up a query engine that can retrieve relevant information and generate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582efce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector index from documents\n",
    "print(\"üèóÔ∏è Building vector index from documents...\")\n",
    "print(\"This may take a few minutes as we embed all text chunks...\")\n",
    "\n",
    "try:\n",
    "    # Create the vector index\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        service_context=service_context,\n",
    "        storage_context=storage_context,\n",
    "        show_progress=True\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Vector index created successfully!\")\n",
    "    print(f\"üìä Index contains {faiss_index.ntotal} vectors\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating index: {e}\")\n",
    "    # Create a simpler index without custom components\n",
    "    print(\"üí° Trying with default settings...\")\n",
    "    index = VectorStoreIndex.from_documents(documents, show_progress=True)\n",
    "    print(\"‚úÖ Fallback index created!\")\n",
    "\n",
    "# Create query engine\n",
    "TOP_K = 5  # Number of most relevant chunks to retrieve\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=TOP_K,\n",
    "    response_mode=\"compact\"  # Combines retrieved chunks efficiently\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Query engine ready!\")\n",
    "print(f\"üîç Configuration:\")\n",
    "print(f\"   - Retrieval: Top {TOP_K} most relevant chunks\")\n",
    "print(f\"   - Response mode: Compact (efficient combination)\")\n",
    "print(f\"   - Vector store: Faiss with {faiss_index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbc8bfb",
   "metadata": {},
   "source": [
    "## 8. Create Chatbot Interface\n",
    "\n",
    "Now let's create an interactive chatbot function that can answer questions about your course notes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c5742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_chatbot(question: str, show_sources: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Ask the chatbot a question and display the answer with optional source information\n",
    "    \n",
    "    Args:\n",
    "        question: The question to ask\n",
    "        show_sources: Whether to show source information\n",
    "    \"\"\"\n",
    "    print(f\"ü§î Question: {question}\")\n",
    "    print(\"ü§ñ Thinking...\")\n",
    "    \n",
    "    try:\n",
    "        # Get response from query engine\n",
    "        response = query_engine.query(question)\n",
    "        \n",
    "        # Display answer\n",
    "        print(f\"\\nüí° Answer:\")\n",
    "        print(f\"{response}\")\n",
    "        \n",
    "        # Show source information if requested\n",
    "        if show_sources and hasattr(response, 'source_nodes'):\n",
    "            print(f\"\\nüìö Sources ({len(response.source_nodes)} chunks used):\")\n",
    "            for i, node in enumerate(response.source_nodes[:3], 1):  # Show top 3 sources\n",
    "                filename = node.metadata.get('filename', 'Unknown')\n",
    "                score = getattr(node, 'score', 'N/A')\n",
    "                print(f\"   {i}. {filename} (similarity: {score})\")\n",
    "                print(f\"      Preview: {node.text[:100]}...\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Create a simple chatbot class for easier use\n",
    "class CourseNotesChatbot:\n",
    "    \"\"\"Simple chatbot class for course notes Q&A\"\"\"\n",
    "    \n",
    "    def __init__(self, query_engine):\n",
    "        self.query_engine = query_engine\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def ask(self, question: str) -> str:\n",
    "        \"\"\"Ask a question and return the answer\"\"\"\n",
    "        try:\n",
    "            response = self.query_engine.query(question)\n",
    "            answer = str(response)\n",
    "            \n",
    "            # Store in conversation history\n",
    "            self.conversation_history.append({\n",
    "                'question': question,\n",
    "                'answer': answer,\n",
    "                'timestamp': pd.Timestamp.now()\n",
    "            })\n",
    "            \n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            return f\"Sorry, I encountered an error: {e}\"\n",
    "    \n",
    "    def get_conversation_history(self) -> pd.DataFrame:\n",
    "        \"\"\"Get conversation history as DataFrame\"\"\"\n",
    "        return pd.DataFrame(self.conversation_history)\n",
    "\n",
    "# Initialize chatbot\n",
    "chatbot = CourseNotesChatbot(query_engine)\n",
    "\n",
    "print(\"üéâ Chatbot is ready!\")\n",
    "print(\"üí¨ You can now ask questions using: ask_chatbot('your question here')\")\n",
    "print(\"üîß Or use the chatbot object: chatbot.ask('your question')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340b0ee5",
   "metadata": {},
   "source": [
    "## 9. Test the Chatbot with Sample Questions\n",
    "\n",
    "Let's test our chatbot with various questions to see how well it can answer from the course notes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3af96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Questions - demonstrating different types of queries\n",
    "test_questions = [\n",
    "    \"What is machine learning?\",\n",
    "    \"What are the types of machine learning?\", \n",
    "    \"Explain overfitting and underfitting\",\n",
    "    \"What is the bias-variance tradeoff?\",\n",
    "    \"List the popular machine learning algorithms mentioned\",\n",
    "    \"What are some practical applications of machine learning?\",\n",
    "    \"Define precision and recall\",\n",
    "    \"What is gradient descent?\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing chatbot with sample questions...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test each question\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\nüìù Test {i}/{len(test_questions)}\")\n",
    "    ask_chatbot(question, show_sources=False)\n",
    "    \n",
    "print(\"\\nüéØ All tests completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971dfeaf",
   "metadata": {},
   "source": [
    "### Interactive Testing\n",
    "\n",
    "You can also ask your own questions by running the cells below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c32b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own questions here!\n",
    "# Modify the question below and run the cell\n",
    "\n",
    "your_question = \"What is the difference between supervised and unsupervised learning?\"\n",
    "ask_chatbot(your_question, show_sources=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664c93e9",
   "metadata": {},
   "source": [
    "## 10. Save and Export the Chatbot\n",
    "\n",
    "Let's save our chatbot configuration and provide instructions for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d2fc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the index for future use\n",
    "vector_store_dir = Path(\"../vector_store\")\n",
    "vector_store_dir.mkdir(exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Save the index\n",
    "    index.storage_context.persist(persist_dir=str(vector_store_dir))\n",
    "    print(\"‚úÖ Index saved successfully!\")\n",
    "    print(f\"üìÅ Location: {vector_store_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not save index: {e}\")\n",
    "\n",
    "# Create a summary of the chatbot configuration\n",
    "config_summary = {\n",
    "    \"embedding_model\": EMBEDDING_MODEL,\n",
    "    \"llm_model\": LLM_MODEL,\n",
    "    \"chunk_size\": CHUNK_SIZE,\n",
    "    \"chunk_overlap\": CHUNK_OVERLAP,\n",
    "    \"top_k_retrieval\": TOP_K,\n",
    "    \"vector_dimension\": EMBEDDING_DIM,\n",
    "    \"total_documents\": len(documents),\n",
    "    \"total_chunks\": len(nodes) if 'nodes' in locals() else 0,\n",
    "    \"vector_count\": faiss_index.ntotal\n",
    "}\n",
    "\n",
    "print(f\"\\nüìä Chatbot Configuration Summary:\")\n",
    "for key, value in config_summary.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Save configuration to file\n",
    "config_file = Path(\"../chatbot_config.json\")\n",
    "import json\n",
    "with open(config_file, 'w') as f:\n",
    "    json.dump(config_summary, f, indent=2)\n",
    "print(f\"\\nüíæ Configuration saved to: {config_file}\")\n",
    "\n",
    "# Display conversation history if any questions were asked\n",
    "if hasattr(chatbot, 'conversation_history') and chatbot.conversation_history:\n",
    "    print(f\"\\nüí¨ Conversation History:\")\n",
    "    history_df = chatbot.get_conversation_history()\n",
    "    print(history_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836f79c0",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You have successfully built a complete RAG-based chatbot that can answer questions from your course notes!\n",
    "\n",
    "### What You've Accomplished\n",
    "\n",
    "‚úÖ **Document Processing**: Loaded and processed course notes from multiple formats  \n",
    "‚úÖ **Text Chunking**: Split documents into optimal chunks for retrieval  \n",
    "‚úÖ **Vector Embeddings**: Created semantic embeddings using Hugging Face models  \n",
    "‚úÖ **Vector Database**: Set up Faiss for fast similarity search  \n",
    "‚úÖ **Language Model**: Integrated a language model for response generation  \n",
    "‚úÖ **RAG Pipeline**: Built a complete Retrieval-Augmented Generation system  \n",
    "‚úÖ **Interactive Interface**: Created functions to interact with the chatbot  \n",
    "‚úÖ **Testing**: Tested the chatbot with various types of questions  \n",
    "\n",
    "### Key Technical Components\n",
    "\n",
    "1. **LlamaIndex**: Document indexing and retrieval framework\n",
    "2. **Faiss**: High-performance vector similarity search\n",
    "3. **Hugging Face Transformers**: Pre-trained models for embeddings and text generation\n",
    "4. **RAG Architecture**: Combines retrieval and generation for accurate answers\n",
    "\n",
    "### Assignment Submission Checklist\n",
    "\n",
    "- ‚úÖ **Code Files**: Complete implementation with comments\n",
    "- ‚úÖ **Requirements**: All dependencies listed in requirements.txt  \n",
    "- ‚úÖ **Documentation**: Comprehensive README and code documentation\n",
    "- ‚úÖ **Demonstration**: This notebook serves as your demo\n",
    "- ‚úÖ **Testing**: Multiple test questions answered successfully\n",
    "\n",
    "### Next Steps for Your Assignment\n",
    "\n",
    "1. **Replace Sample Notes**: Add your actual course notes to the `course_notes/` folder\n",
    "2. **Test Thoroughly**: Ask questions covering all your course topics\n",
    "3. **Take Screenshots**: Capture the chatbot answering at least 5 different questions\n",
    "4. **Write Report**: Document your approach, challenges, and learnings\n",
    "5. **Create Submission**: Package everything according to submission guidelines\n",
    "\n",
    "### How to Use Your Chatbot\n",
    "\n",
    "```python\n",
    "# In a new session, you can recreate the chatbot by running all cells above\n",
    "# Or use the standalone files in the code/ directory:\n",
    "\n",
    "# Command line:\n",
    "# python code/chatbot.py\n",
    "\n",
    "# Web interface:\n",
    "# streamlit run code/streamlit_app.py\n",
    "```\n",
    "\n",
    "### Reflection\n",
    "\n",
    "This assignment demonstrates the power of RAG systems for creating domain-specific chatbots. By combining document retrieval with language generation, we can create chatbots that provide accurate, contextual answers based on specific knowledge sources - in this case, your course notes!\n",
    "\n",
    "**What makes this chatbot special:**\n",
    "- It only answers based on your course content\n",
    "- It shows source information for transparency  \n",
    "- It handles various document formats\n",
    "- It uses state-of-the-art NLP techniques\n",
    "- It's fully customizable and extensible\n",
    "\n",
    "Great job building your course notes chatbot! üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
