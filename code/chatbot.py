"""
Course Notes Chatbot - Main Implementation
Uses LlamaIndex, Faiss, and Hugging Face transformers for RAG-based question answering
"""

import logging
import os
from pathlib import Path
from typing import List, Optional
import json

# LlamaIndex imports - using correct structure
from llama_index.core import VectorStoreIndex, StorageContext, Document, Settings
from llama_index.core.node_parser import SentenceSplitter

# Try to import embeddings - with fallback to default
HuggingFaceEmbedding = None
try:
    from llama_index_embeddings_huggingface import HuggingFaceEmbedding
except ImportError:
    try:
        from llama_index.embeddings.huggingface import HuggingFaceEmbedding
    except ImportError:
        pass

# Optional imports with fallbacks
try:
    from llama_index.vector_stores.faiss import FaissVectorStore
except ImportError:
    FaissVectorStore = None

# Local imports
from config import Config
from document_loader import DocumentLoader

# Replace direct faiss import with optional import and graceful fallback
try:
    import faiss  # type: ignore
except ImportError:
    faiss = None
    logging.getLogger(__name__).warning(
        "faiss not installed (pip install faiss-cpu). Falling back to default vector store."
    )

# Document class is already imported above

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CourseNotesChatbot:
    """Main chatbot class that handles document indexing and question answering"""
    
    def __init__(self):
        """Initialize the chatbot with configuration"""
        self.config = Config()
        self.config.ensure_directories()
        
        self.document_loader = DocumentLoader()
        self.index = None
        self.query_engine = None
        
        # Initialize components
        self._setup_embedding_model()
        self._setup_llm()
        
    def _setup_embedding_model(self):
        """Setup the embedding model with graceful fallbacks"""
        logger.info("Setting up embedding model...")
        
        # Try HuggingFaceEmbedding if available
        if HuggingFaceEmbedding is not None:
            try:
                self.embed_model = HuggingFaceEmbedding(
                    model_name=self.config.EMBEDDING_MODEL
                )
                logger.info(f"Embedding model loaded: {self.config.EMBEDDING_MODEL}")
                return
            except Exception as e:
                logger.warning(f"HuggingFaceEmbedding failed: {e}")
        
        # Fallback: Use default embedding (None will use LlamaIndex default)
        logger.info("Using default embedding model")
        self.embed_model = None
    
    def _setup_llm(self):
        """Setup the language model - using a simple approach"""
        logger.info("Setting up language model...")
        try:
            # Use a lightweight model that works well for Q&A
            from transformers import pipeline
            
            # Create a simple Q&A pipeline
            self.qa_pipeline = pipeline(
                "question-answering",
                model="distilbert-base-cased-distilled-squad",
                tokenizer="distilbert-base-cased-distilled-squad"
            )
            
            # For LlamaIndex compatibility, we'll use None and handle Q&A separately
            self.llm = None
            logger.info("Q&A pipeline loaded successfully")
        except Exception as e:
            logger.error(f"Error loading language model: {e}")
            self.qa_pipeline = None
            self.llm = None
    
    def load_documents(self) -> bool:
        """Load all course notes documents"""
        logger.info("Loading course notes documents...")
        
        documents = self.document_loader.load_all_documents(self.config.COURSE_NOTES_DIR)
        
        if not documents:
            logger.warning("No documents found in course_notes directory!")
            return False
        
        logger.info(f"Loaded {len(documents)} document chunks")
        self.documents = documents
        return True
    
    def create_index(self):
        """Create vector index from loaded documents"""
        if not hasattr(self, 'documents') or not self.documents:
            logger.error("No documents loaded. Call load_documents() first.")
            return False
        
        logger.info("Creating vector index...")
        
        try:
            # Create node parser
            node_parser = SentenceSplitter(
                chunk_size=self.config.CHUNK_SIZE,
                chunk_overlap=self.config.CHUNK_OVERLAP
            )
            
            # Configure settings instead of ServiceContext
            if self.embed_model is not None:
                Settings.embed_model = self.embed_model
            else:
                # Use local embeddings to avoid OpenAI API requirement
                Settings.embed_model = "local"
            Settings.node_parser = node_parser
            
            # Try to auto-detect embedding dimension
            d = 384  # Safe default
            if self.embed_model is not None:
                try:
                    d = len(self.embed_model.get_text_embedding("dimension probe"))
                    logger.info(f"Detected embedding dimension: {d}")
                except Exception:
                    logger.warning(f"Could not detect embedding dimension. Using default d={d}.")
            else:
                logger.info(f"Using default embedding dimension: {d}")

            # Build index with Faiss if available; otherwise use default in-memory store
            if faiss is not None and FaissVectorStore is not None:
                try:
                    faiss_index = faiss.IndexFlatL2(d)
                    vector_store = FaissVectorStore(faiss_index=faiss_index)
                    storage_context = StorageContext.from_defaults(vector_store=vector_store)
                    self.index = VectorStoreIndex.from_documents(
                        self.documents,
                        storage_context=storage_context,
                        show_progress=True
                    )
                    logger.info("Vector index created successfully with Faiss!")
                except Exception as e:
                    logger.warning(f"Faiss setup failed ({e}). Falling back to default vector store.")
                    self.index = VectorStoreIndex.from_documents(
                        self.documents,
                        show_progress=True
                    )
            else:
                self.index = VectorStoreIndex.from_documents(
                    self.documents,
                    show_progress=True
                )
                logger.info("Vector index created with default vector store (no Faiss).")
            
            return True
            
        except Exception as e:
            logger.error(f"Error creating index: {e}")
            return False
    
    def setup_query_engine(self):
        """Setup the query engine for answering questions"""
        if not self.index:
            logger.error("No index available. Create index first.")
            return False
        
        try:
            # Create a simple retriever instead of query engine to avoid LLM requirements
            self.retriever = self.index.as_retriever(
                similarity_top_k=self.config.TOP_K_RESULTS
            )
            self.query_engine = None  # We'll handle queries manually
            logger.info("Retriever setup complete!")
            return True
        except Exception as e:
            logger.error(f"Error setting up retriever: {e}")
            return False
    
    def ask_question(self, question: str) -> str:
        """Ask a question and get an answer from the course notes"""
        if not self.retriever:
            return "Chatbot not properly initialized. Please setup the index first."
        
        try:
            logger.info(f"Processing question: {question}")
            # Retrieve relevant documents
            retrieved_nodes = self.retriever.retrieve(question)
            
            if not retrieved_nodes:
                return "I couldn't find relevant information in your course notes to answer this question."
            
            # Get the full course notes text for comprehensive answers
            full_text = retrieved_nodes[0].text
            sources = set()
            for node in retrieved_nodes:
                sources.add(node.metadata.get('filename', 'course notes'))
            
            # Create topic-based answer extraction
            question_lower = question.lower()
            answer = self._extract_topic_based_answer(question_lower, full_text)
            
            source_list = ", ".join(sources)
            return f"{answer}\n\n(Sources: {source_list})"
            
        except Exception as e:
            logger.error(f"Error processing question: {e}")
            return f"Sorry, I encountered an error while processing your question: {str(e)}"
    
    def _extract_topic_based_answer(self, question_lower: str, full_text: str) -> str:
        """Extract comprehensive answers based on question topics"""
        
        # Split text into sections
        sections = full_text.split('\n\n')
        
        # Topic mappings for better answer extraction
        if any(word in question_lower for word in ['what is machine learning', 'define machine learning', 'machine learning work']):
            # Find Introduction section
            answer_parts = []
            for section in sections:
                if ('machine learning' in section.lower() and 
                    any(key in section.lower() for key in ['introduction', 'subset of artificial', 'algorithms and statistical'])):
                    answer_parts.append(section.strip())
            return "Based on your course notes:\n\n" + "\n\n".join(answer_parts[:3]) if answer_parts else self._get_fallback_answer(question_lower, sections)
        
        elif any(word in question_lower for word in ['types of machine learning', 'different types', 'supervised learning', 'unsupervised']):
            # Find Types section
            answer_parts = []
            for section in sections:
                if (('types of machine learning' in section.lower() or 'supervised learning' in section.lower() or 
                     'unsupervised learning' in section.lower() or 'reinforcement learning' in section.lower()) and
                    len(section.strip()) > 50):
                    answer_parts.append(section.strip())
            return "Based on your course notes:\n\n" + "\n\n".join(answer_parts[:4]) if answer_parts else self._get_fallback_answer(question_lower, sections)
        
        elif any(word in question_lower for word in ['overfitting', 'underfitting', 'bias', 'variance']):
            # Find definitions section
            answer_parts = []
            for section in sections:
                if any(key in section.lower() for key in ['overfitting', 'underfitting', 'bias-variance', 'important definitions']):
                    answer_parts.append(section.strip())
            return "Based on your course notes:\n\n" + "\n\n".join(answer_parts[:3]) if answer_parts else self._get_fallback_answer(question_lower, sections)
        
        elif any(word in question_lower for word in ['data preprocessing', 'preprocessing', 'data cleaning', 'feature']):
            # Find preprocessing section
            answer_parts = []
            for section in sections:
                if any(key in section.lower() for key in ['data preprocessing', 'data cleaning', 'feature scaling', 'feature selection']):
                    answer_parts.append(section.strip())
            return "Based on your course notes:\n\n" + "\n\n".join(answer_parts[:3]) if answer_parts else self._get_fallback_answer(question_lower, sections)
        
        elif any(word in question_lower for word in ['evaluation', 'metrics', 'precision', 'recall', 'accuracy', 'f1']):
            # Find evaluation section
            answer_parts = []
            for section in sections:
                if any(key in section.lower() for key in ['model evaluation', 'precision', 'recall', 'accuracy', 'f1-score']):
                    answer_parts.append(section.strip())
            return "Based on your course notes:\n\n" + "\n\n".join(answer_parts[:3]) if answer_parts else self._get_fallback_answer(question_lower, sections)
        
        else:
            # General fallback - find most relevant sections
            return self._get_fallback_answer(question_lower, sections)
    
    def _get_fallback_answer(self, question_lower: str, sections: list) -> str:
        """Fallback method to find relevant sections"""
        question_words = set(word for word in question_lower.split() if len(word) > 3)
        scored_sections = []
        
        for section in sections:
            if len(section.strip()) < 20:  # Skip very short sections
                continue
            section_lower = section.lower()
            score = sum(1 for word in question_words if word in section_lower)
            if score > 0:
                scored_sections.append((score, section.strip()))
        
        # Sort by relevance score and return top sections
        scored_sections.sort(key=lambda x: x[0], reverse=True)
        relevant_sections = [section for score, section in scored_sections[:3]]
        
        if relevant_sections:
            return "Based on your course notes:\n\n" + "\n\n".join(relevant_sections)
        else:
            # Last resort - return first meaningful sections
            meaningful_sections = [s.strip() for s in sections if len(s.strip()) > 50][:3]
            return "Based on your course notes:\n\n" + "\n\n".join(meaningful_sections)
    
    def initialize_chatbot(self) -> bool:
        """Initialize the complete chatbot system"""
        logger.info("Initializing Course Notes Chatbot...")
        
        # Step 1: Load documents
        if not self.load_documents():
            logger.error("Failed to load documents")
            return False
        
        # Step 2: Create index
        if not self.create_index():
            logger.error("Failed to create index")
            return False
        
        # Step 3: Setup query engine
        if not self.setup_query_engine():
            logger.error("Failed to setup retriever")
            return False
        
        logger.info("Chatbot initialization complete!")
        return True
    
    def save_index(self, path: Optional[str] = None):
        """Save the index to disk"""
        if not self.index:
            logger.error("No index to save")
            return False
        
        try:
            save_path = path or str(self.config.VECTOR_STORE_DIR)
            self.index.storage_context.persist(persist_dir=save_path)
            logger.info(f"Index saved to {save_path}")
            return True
        except Exception as e:
            logger.error(f"Error saving index: {e}")
            return False
    
    def load_index(self, path: Optional[str] = None):
        """Load a previously saved index"""
        try:
            load_path = path or str(self.config.VECTOR_STORE_DIR)
            if not Path(load_path).exists():
                logger.warning(f"Index path {load_path} does not exist")
                return False
            
            # This would require implementing proper index loading
            # For now, we'll recreate the index
            logger.info("Loading saved index not implemented, recreating...")
            return self.initialize_chatbot()
            
        except Exception as e:
            logger.error(f"Error loading index: {e}")
            return False
    
    def get_document_info(self) -> dict:
        """Get information about loaded documents"""
        if not hasattr(self, 'documents'):
            return {"error": "No documents loaded"}
        
        info = {
            "total_documents": len(self.documents),
            "documents": []
        }
        
        for doc in self.documents:
            doc_info = {
                "filename": doc.metadata.get("filename", "Unknown"),
                "file_type": doc.metadata.get("file_type", "Unknown"),
                "content_length": len(doc.text)
            }
            info["documents"].append(doc_info)
        
        return info

def main():
    """Main function to run the chatbot interactively"""
    print("ğŸ¤– Course Notes Chatbot")
    print("=" * 50)
    
    # Initialize chatbot
    chatbot = CourseNotesChatbot()
    
    # Check if course_notes directory exists
    if not chatbot.config.COURSE_NOTES_DIR.exists():
        print("âš ï¸  Course notes directory not found!")
        print(f"Creating directory: {chatbot.config.COURSE_NOTES_DIR}")
        chatbot.config.COURSE_NOTES_DIR.mkdir(parents=True, exist_ok=True)
    
    # Check if documents exist
    course_notes_files = list(chatbot.config.COURSE_NOTES_DIR.glob("*"))
    if not course_notes_files:
        print("âš ï¸  No course notes found!")
        print(f"Please add your course notes to: {chatbot.config.COURSE_NOTES_DIR}")
        print("Supported formats: .txt, .pdf, .docx, .md, .xlsx")
        print("\nğŸ“ Example commands to add files:")
        print(f"   cp /path/to/your/notes.pdf {chatbot.config.COURSE_NOTES_DIR}/")
        print(f"   cp /path/to/your/lecture.txt {chatbot.config.COURSE_NOTES_DIR}/")
        print("\nğŸ”„ After adding files, run this script again!")
        return
    
    print(f"ğŸ“ Found {len(course_notes_files)} files in course_notes directory")
    print("ğŸ“š Initializing chatbot with your course notes...")
    
    if not chatbot.initialize_chatbot():
        print("âŒ Failed to initialize chatbot")
        print("\nğŸ”§ Troubleshooting tips:")
        print("1) Install dependencies: pip install -r requirements.txt")
        print("2) If Faiss missing: pip install faiss-cpu  (optional)")
        print("3) If embeddings fail on torch: pip install torch transformers sentence-transformers")
        print("   - Or use CPU-friendly fallback: pip install fastembed")
        print("4) Try smaller files or reduce CHUNK_SIZE in code/config.py")
        print("5) Re-run setup check: python3 setup_check.py")
        return
    
    print("âœ… Chatbot ready!")
    print("\nDocument Info:")
    doc_info = chatbot.get_document_info()
    print(f"ğŸ“„ Loaded {doc_info['total_documents']} document chunks")
    
    for doc in doc_info.get('documents', [])[:5]:  # Show first 5 files
        filename = doc.get('filename', 'Unknown')
        file_type = doc.get('file_type', 'Unknown')
        print(f"   â€¢ {filename} ({file_type})")
    
    if len(doc_info.get('documents', [])) > 5:
        print(f"   ... and {len(doc_info['documents']) - 5} more files")
    
    print("\n" + "=" * 50)
    print("ğŸ’¬ You can now ask questions about your course notes!")
    print("Type 'quit' or 'exit' to stop")
    print("=" * 50)
    
    # Interactive chat loop
    while True:
        try:
            question = input("\nğŸ¤” Your question: ").strip()
            
            if question.lower() in ['quit', 'exit', 'bye']:
                print("ğŸ‘‹ Goodbye!")
                break
            
            if not question:
                continue
            
            if question.lower() in ['help', '?']:
                print("\nğŸ’¡ Tips:")
                print("â€¢ Ask specific questions about your course material")
                print("â€¢ Be clear and concise in your questions")
                print("â€¢ The chatbot searches through all your uploaded notes")
                print("â€¢ Type 'quit' or 'exit' to stop")
                continue
            
            print("\nğŸ¤– Thinking...")
            answer = chatbot.ask_question(question)
            print(f"\nğŸ’¡ Answer:\n{answer}")
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ Goodbye!")
            break
        except Exception as e:
            print(f"\nâŒ Error: {e}")
            print("ğŸ’¡ Try rephrasing your question or type 'help' for tips")

if __name__ == "__main__":
    main()